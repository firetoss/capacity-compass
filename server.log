INFO:     Started server process [66103]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://127.0.0.1:8080 (Press CTRL+C to quit)
DEBUG:capacity_compass.pipeline.normalizer:normalizing request model=Qwen3-8B-Instruct precision=bf16 context=12000 vendors=['NVIDIA', 'Huawei']
DEBUG:capacity_compass.pipeline.normalizer:model Qwen3-8B-Instruct resolved directly to Qwen/Qwen3-8B
DEBUG:capacity_compass.pipeline.normalizer:normalized model=Qwen3-8B-Instruct precision=bf16 context=12000 vendor_scope=['NVIDIA', 'Huawei'] anonymous=False
INFO:capacity_compass.pipeline.service:normalized request model=Qwen3-8B-Instruct precision=bf16 context=12000
DEBUG:capacity_compass.pipeline.requirements:requirements model=Qwen3-8B-Instruct preset=对话型助手 ctx=12000 weights=16.00 GB kv=28.31 GB total=50.96 GB compute=768.00 Tx
INFO:capacity_compass.pipeline.hardware_filter:hardware filter vendors=['NVIDIA', 'Huawei'] precision=bf16 weights=16.00 GB -> 4 eligible (raw=4)
WARNING:capacity_compass.pipeline.card_sizer:compute spec missing for gpu=NVIDIA A10 24GB precision=bf16; relying on memory-bound sizing
DEBUG:capacity_compass.pipeline.card_sizer:size_cards gpu=NVIDIA A10 24GB precision=bf16 cards_mem=3 cards_compute=None cards_needed=3 headroom=0.29
WARNING:capacity_compass.pipeline.card_sizer:compute spec missing for gpu=NVIDIA A100 40GB precision=bf16; relying on memory-bound sizing
DEBUG:capacity_compass.pipeline.card_sizer:size_cards gpu=NVIDIA A100 40GB precision=bf16 cards_mem=2 cards_compute=None cards_needed=2 headroom=0.36
WARNING:capacity_compass.pipeline.card_sizer:compute spec missing for gpu=NVIDIA L20 48GB precision=bf16; relying on memory-bound sizing
DEBUG:capacity_compass.pipeline.card_sizer:size_cards gpu=NVIDIA L20 48GB precision=bf16 cards_mem=2 cards_compute=None cards_needed=2 headroom=0.47
WARNING:capacity_compass.pipeline.card_sizer:compute spec missing for gpu=NVIDIA H20 96GB precision=bf16; relying on memory-bound sizing
DEBUG:capacity_compass.pipeline.card_sizer:size_cards gpu=NVIDIA H20 96GB precision=bf16 cards_mem=1 cards_compute=None cards_needed=1 headroom=0.47
INFO:capacity_compass.pipeline.recommender:top candidate gpu=NVIDIA H20 96GB cards=1 price=None deploy=excellent
INFO:capacity_compass.pipeline.service:scene=chat top_gpu=NVIDIA H20 96GB cards=1 vendors=['NVIDIA', 'Huawei']
DEBUG:capacity_compass.pipeline.requirements:requirements model=Qwen3-8B-Instruct preset=检索增强问答（RAG） ctx=32768 weights=16.00 GB kv=38.65 GB total=62.85 GB compute=524.29 Tx
INFO:capacity_compass.pipeline.hardware_filter:hardware filter vendors=['NVIDIA', 'Huawei'] precision=bf16 weights=16.00 GB -> 4 eligible (raw=4)
WARNING:capacity_compass.pipeline.card_sizer:compute spec missing for gpu=NVIDIA A10 24GB precision=bf16; relying on memory-bound sizing
DEBUG:capacity_compass.pipeline.card_sizer:size_cards gpu=NVIDIA A10 24GB precision=bf16 cards_mem=3 cards_compute=None cards_needed=3 headroom=0.13
WARNING:capacity_compass.pipeline.card_sizer:compute spec missing for gpu=NVIDIA A100 40GB precision=bf16; relying on memory-bound sizing
DEBUG:capacity_compass.pipeline.card_sizer:size_cards gpu=NVIDIA A100 40GB precision=bf16 cards_mem=2 cards_compute=None cards_needed=2 headroom=0.21
WARNING:capacity_compass.pipeline.card_sizer:compute spec missing for gpu=NVIDIA L20 48GB precision=bf16; relying on memory-bound sizing
DEBUG:capacity_compass.pipeline.card_sizer:size_cards gpu=NVIDIA L20 48GB precision=bf16 cards_mem=2 cards_compute=None cards_needed=2 headroom=0.35
WARNING:capacity_compass.pipeline.card_sizer:compute spec missing for gpu=NVIDIA H20 96GB precision=bf16; relying on memory-bound sizing
DEBUG:capacity_compass.pipeline.card_sizer:size_cards gpu=NVIDIA H20 96GB precision=bf16 cards_mem=1 cards_compute=None cards_needed=1 headroom=0.35
INFO:capacity_compass.pipeline.recommender:top candidate gpu=NVIDIA H20 96GB cards=1 price=None deploy=excellent
INFO:capacity_compass.pipeline.service:scene=rag top_gpu=NVIDIA H20 96GB cards=1 vendors=['NVIDIA', 'Huawei']
DEBUG:capacity_compass.pipeline.requirements:requirements model=Qwen3-8B-Instruct preset=长文生成 ctx=12000 weights=16.00 GB kv=7.08 GB total=26.54 GB compute=64.00 Tx
INFO:capacity_compass.pipeline.hardware_filter:hardware filter vendors=['NVIDIA', 'Huawei'] precision=bf16 weights=16.00 GB -> 4 eligible (raw=4)
WARNING:capacity_compass.pipeline.card_sizer:compute spec missing for gpu=NVIDIA A10 24GB precision=bf16; relying on memory-bound sizing
DEBUG:capacity_compass.pipeline.card_sizer:size_cards gpu=NVIDIA A10 24GB precision=bf16 cards_mem=2 cards_compute=None cards_needed=2 headroom=0.45
WARNING:capacity_compass.pipeline.card_sizer:compute spec missing for gpu=NVIDIA A100 40GB precision=bf16; relying on memory-bound sizing
DEBUG:capacity_compass.pipeline.card_sizer:size_cards gpu=NVIDIA A100 40GB precision=bf16 cards_mem=1 cards_compute=None cards_needed=1 headroom=0.34
WARNING:capacity_compass.pipeline.card_sizer:compute spec missing for gpu=NVIDIA L20 48GB precision=bf16; relying on memory-bound sizing
DEBUG:capacity_compass.pipeline.card_sizer:size_cards gpu=NVIDIA L20 48GB precision=bf16 cards_mem=1 cards_compute=None cards_needed=1 headroom=0.45
WARNING:capacity_compass.pipeline.card_sizer:compute spec missing for gpu=NVIDIA H20 96GB precision=bf16; relying on memory-bound sizing
DEBUG:capacity_compass.pipeline.card_sizer:size_cards gpu=NVIDIA H20 96GB precision=bf16 cards_mem=1 cards_compute=None cards_needed=1 headroom=0.72
INFO:capacity_compass.pipeline.recommender:top candidate gpu=NVIDIA A100 40GB cards=1 price=100000.0 deploy=excellent
INFO:capacity_compass.pipeline.service:scene=writer top_gpu=NVIDIA A100 40GB cards=1 vendors=['NVIDIA', 'Huawei']
INFO:capacity_compass.pipeline.evaluation_builder:scene=chat primary_device=NVIDIA H20 96GB cards=1 has_candidates=True
INFO:capacity_compass.pipeline.evaluation_builder:scene=rag primary_device=NVIDIA H20 96GB cards=1 has_candidates=True
INFO:capacity_compass.pipeline.evaluation_builder:scene=writer primary_device=NVIDIA A100 40GB cards=1 has_candidates=True
DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-ad536e5e-4255-49b9-b110-2cdfa2dceb40', 'json_data': {'messages': [{'role': 'system', 'content': '系统角色\n你是一名售前顾问，面向非技术客户，用通俗中文给出“算力/设备推荐”的快速参考说明。不要出现专业术语（如 TFLOPS、KV、MoE 等），以“设备、卡数、预计体验、提示”进行表达。\n\n写作要求\n- 仅输出可直接展示的中文，不输出 JSON 或解释过程。\n- 结构固定三段，句子简短，避免术语。\n- 不做保证或承诺，加入“仅供参考、以压测为准”的提示。\n- 如果发生模型切换或上下文截断，用一句话说明原因。\n\n可用材料（JSON 摘要）\n- quick_answer：主推/备选/提示/置信度/可能的 switch_notice\n- scenes.chat.sales_summary：对话问答场景（主推/备选/提示）\n- scenes.rag.sales_summary：资料问答场景（主推/备选/提示）\n- scenes.writer.sales_summary：长文写作场景（主推/备选/提示）\n- （可选）scenes.*.guide：每个场景的 1 句用途 + 1 句体验（若提供则引用）\n- disclaimers：通用提示语（可择要引用 1–2 条）\n\n写作框架（严格遵循）\n1) 快速答案\n- 推荐：{chat.primary.device} × {chat.primary.cards}\n- 预计体验：{chat.primary.estimate}\n- 理由：{chat.primary.reason}\n- 备选：列出 1–2 条，格式“{device} × {cards}（{简短理由}）”\n- 提示：从 quick_answer.tips 或通用提示中挑选 1–2 条（如“粗略估算，仅供参考；实际以压测为准”）\n\n2) 常见用法\n- 对话问答：一句用途说明 + 推荐“{chat.primary.device} × {chat.primary.cards}” + 一句体验（例如“反馈较快，日常问答流畅”）\n- 资料问答：一句用途说明 + 推荐“{rag.primary.device} × {rag.primary.cards}” + 一句体验（例如“速度中等，能读懂更长的资料”）\n- 长文写作：一句用途说明 + 推荐“{writer.primary.device} × {writer.primary.cards}” + 一句体验（例如“更稳、更完整的长文生成”）\n\n3) 说明\n- 若发生模型切换：用一句话说明“为更贴合上下文，已切换至 {switch_notice.model} 进行估算”\n- 若上下文超模型上限：用一句话说明“上下文已按模型上限估算”\n- 通用提示：如“价格/生态以服务商确认为准；结果受硬件与网络影响，以压测为准”\n\n注意\n- 若某场景缺少数据，略去该场景，不必占位。\n- 若无备选，省略备选行。\n- 所有品牌/型号按可用材料原样输出，不自行更换。\n\n'}, {'role': 'user', 'content': '{\n  "quick_answer": {\n    "primary": {\n      "device": "NVIDIA H20 96GB",\n      "cards": 1,\n      "estimate": "预计响应：快",\n      "reason": "显存合适、推理稳定"\n    },\n    "alternatives": [\n      {\n        "device": "NVIDIA A100 40GB",\n        "cards": 2,\n        "estimate": "预计响应：快",\n        "reason": "显存合适、推理稳定"\n      },\n      {\n        "device": "NVIDIA L20 48GB",\n        "cards": 2,\n        "estimate": "预计响应：快",\n        "reason": "显存合适、推理稳定"\n      }\n    ],\n    "tips": [\n      "本结果为快速粗略估算，仅供参考；实际以压测为准",\n      "部分设备价格需与服务商确认"\n    ],\n    "confidence": "low",\n    "switch_notice": null\n  },\n  "scenes": {\n    "chat": {\n      "sales_summary": {\n        "primary": {\n          "device": "NVIDIA H20 96GB",\n          "cards": 1,\n          "estimate": "预计响应：快",\n          "reason": "显存合适、推理稳定"\n        },\n        "alternatives": [\n          {\n            "device": "NVIDIA A100 40GB",\n            "cards": 2,\n            "estimate": "预计响应：快",\n            "reason": "显存合适、推理稳定"\n          },\n          {\n            "device": "NVIDIA L20 48GB",\n            "cards": 2,\n            "estimate": "预计响应：快",\n            "reason": "显存合适、推理稳定"\n          }\n        ],\n        "tips": [\n          "本结果为快速粗略估算，仅供参考；实际以压测为准",\n          "部分设备价格需与服务商确认"\n        ],\n        "confidence": "low",\n        "switch_notice": null\n      },\n      "guide": {\n        "title": "对话问答",\n        "fit": "和模型聊天、问问题，互动频繁，每次内容不长",\n        "experience": "反馈较快，日常问答流畅",\n        "tip": "内容特别长时，响应可能变慢"\n      }\n    },\n    "rag": {\n      "sales_summary": {\n        "primary": {\n          "device": "NVIDIA H20 96GB",\n          "cards": 1,\n          "estimate": "预计响应：中",\n          "reason": "显存合适、推理稳定"\n        },\n        "alternatives": [\n          {\n            "device": "NVIDIA A100 40GB",\n            "cards": 2,\n            "estimate": "预计响应：中",\n            "reason": "显存合适、推理稳定"\n          },\n          {\n            "device": "NVIDIA L20 48GB",\n            "cards": 2,\n            "estimate": "预计响应：中",\n            "reason": "显存合适、推理稳定"\n          }\n        ],\n        "tips": [\n          "本结果为快速粗略估算，仅供参考；实际以压测为准",\n          "部分设备价格需与服务商确认"\n        ],\n        "confidence": "low",\n        "switch_notice": null\n      },\n      "guide": {\n        "title": "资料问答",\n        "fit": "基于资料/知识库回答问题，问题背景更长",\n        "experience": "速度中等，能读懂更长的资料",\n        "tip": "资料越长，对设备显存和算力要求越高"\n      }\n    },\n    "writer": {\n      "sales_summary": {\n        "primary": {\n          "device": "NVIDIA A100 40GB",\n          "cards": 1,\n          "estimate": "预计响应：稳",\n          "reason": "显存合适、推理稳定"\n        },\n        "alternatives": [\n          {\n            "device": "NVIDIA H20 96GB",\n            "cards": 1,\n            "estimate": "预计响应：稳",\n            "reason": "显存合适、推理稳定"\n          },\n          {\n            "device": "NVIDIA L20 48GB",\n            "cards": 1,\n            "estimate": "预计响应：稳",\n            "reason": "显存合适、推理稳定"\n          }\n        ],\n        "tips": [\n          "本结果为快速粗略估算，仅供参考；实际以压测为准"\n        ],\n        "confidence": "low",\n        "switch_notice": null\n      },\n      "guide": {\n        "title": "长文写作",\n        "fit": "写报告/方案/营销稿，单次输出较长",\n        "experience": "更稳、更完整的长文生成",\n        "tip": "内容越长，生成时间越久"\n      }\n    }\n  },\n  "model_profile": {\n    "family": "Qwen3",\n    "model_name": "Qwen/Qwen3-8B",\n    "display_name": "Qwen3-8B-Instruct",\n    "aliases": [\n      "Qwen3-8B",\n      "Qwen/Qwen3-8B-Instruct",\n      "Qwen3 8B"\n    ],\n    "modality": "text",\n    "param_count_b": 8.0,\n    "is_moe": false,\n    "vocab_size": 151936,\n    "hidden_size": 4096,\n    "intermediate_size": 12288,\n    "num_hidden_layers": 36,\n    "num_attention_heads": 32,\n    "num_key_value_heads": 8,\n    "head_dim": 128,\n    "max_position_embeddings": 40960,\n    "rope_scaling": null,\n    "tie_word_embeddings": false,\n    "num_experts": null,\n    "top_k": null,\n    "expert_intermediate_size": null,\n    "torch_dtype": "bfloat16",\n    "use_cache": true,\n    "recommended_kv_dtype": "bfloat16",\n    "notes": "结构参数来自 HF Qwen/Qwen3-8B；官方提供 FP8 仓库 Qwen/Qwen3-8B-FP8；官方 AWQ 仓库 Qwen/Qwen3-8B-AWQ。",\n    "supports_fp8": true,\n    "supports_bf16": true,\n    "supports_fp16": true,\n    "supports_int8": null,\n    "quantization_support": [\n      "fp8",\n      "awq_4bit"\n    ]\n  },\n  "disclaimers": [\n    "本评估为快速预估结果，需结合压测和厂商建议后再定型。",\n    "多卡互联、量化策略与软件版本会影响表现，交付前请二次确认。"\n  ]\n}'}], 'model': 'qwen/qwen3-8b'}}
DEBUG:openai._base_client:Sending HTTP Request: POST https://openrouter.ai/api/v1/chat/completions
DEBUG:httpcore.connection:connect_tcp.started host='127.0.0.1' port=7890 local_address=None timeout=30.0 socket_options=None
DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10cf95a90>
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'CONNECT']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'CONNECT']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'CONNECT']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection established', [])
DEBUG:httpcore.proxy:start_tls.started ssl_context=<ssl.SSLContext object at 0x10c9a45f0> server_hostname='openrouter.ai' timeout=30.0
DEBUG:httpcore.proxy:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10cf96150>
DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_headers.complete
DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:send_request_body.complete
DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 14 Nov 2025 13:15:27 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Content-Encoding', b'gzip'), (b'Access-Control-Allow-Origin', b'*'), (b'Vary', b'Accept-Encoding'), (b'Permissions-Policy', b'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")'), (b'Referrer-Policy', b'no-referrer, strict-origin-when-cross-origin'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'99e6c710d9146576-AMS')])
INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions "HTTP/1.1 200 OK"
DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>
DEBUG:httpcore.http11:receive_response_body.complete
DEBUG:httpcore.http11:response_closed.started
DEBUG:httpcore.http11:response_closed.complete
DEBUG:openai._base_client:HTTP Response: POST https://openrouter.ai/api/v1/chat/completions "200 OK" Headers({'date': 'Fri, 14 Nov 2025 13:15:27 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'content-encoding': 'gzip', 'access-control-allow-origin': '*', 'vary': 'Accept-Encoding', 'permissions-policy': 'payment=(self "https://checkout.stripe.com" "https://connect-js.stripe.com" "https://js.stripe.com" "https://*.js.stripe.com" "https://hooks.stripe.com")', 'referrer-policy': 'no-referrer, strict-origin-when-cross-origin', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '99e6c710d9146576-AMS'})
DEBUG:openai._base_client:request_id: None
INFO:     127.0.0.1:50258 - "POST /api/llm/capacity/evaluate HTTP/1.1" 200 OK
