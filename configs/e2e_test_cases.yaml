# 端到端测试用例定义（用于验证接口整体行为，而非精确数值）
# 说明：
# - expected_* 字段并非要求完全相等，而是用作断言条件（例如 cards_needed 不大于某值、优选方案包含指定 GPU 等）。
# - LLM 生成的说明文本不在此处做强约束，只要结构字段满足即可。

- id: tc_qwen3_8b_chat_fp16
  description: "Qwen3-8B FP16，典型 chat 场景，期望单卡 H20/A100 类设备可以满足。"
  request:
    model_name: "Qwen3-8B-Instruct"
    param_count_b: 8
    max_context_len: 8192
    precision: "fp16"
    vendor_scope: ["NVIDIA", "Huawei", "Kunlunxin"]
  expectations:
    scenarios:
      chat:
        # 期望 chat 场景存在且有推荐结果
        has_recommendation: true
        primary_contains_any_gpu_id:
          - "nvidia_h20_96g"
          - "nvidia_a100_40g"
      rag:
        has_recommendation: true
      writer:
        has_recommendation: true

- id: tc_deepseek_r1_distill_7b_chat_bf16
  description: "DeepSeek-R1-Distill-Qwen-7B BF16，chat 场景，期望大量卡可用。"
  request:
    model_name: "DeepSeek-R1-Distill-Qwen-7B"
    param_count_b: 7
    max_context_len: 8192
    precision: "bf16"
    vendor_scope: ["NVIDIA", "Huawei", "Kunlunxin"]
  expectations:
    scenarios:
      chat:
        has_recommendation: true
        # 预期至少有 1 卡方案，cards_needed 不应超过 4 张
        max_cards_needed: 4

- id: tc_qwen3_vl_7b_rag_multimodal
  description: "Qwen3-VL-7B-Instruct，多模态模型，RAG 场景估算应使用 compute_multiplier。"
  request:
    model_name: "Qwen3-VL-7B-Instruct"
    param_count_b: 7
    max_context_len: 32768
    precision: "fp16"
    vendor_scope: ["NVIDIA", "Huawei"]
  expectations:
    scenarios:
      rag:
        has_recommendation: true
        # 校验内部逻辑是否识别为 text_vision 模态，可通过日志或调试字段验证
        modality: "text_vision"

- id: tc_unknown_model_by_param_only
  description: "未知模型，仅给出参数量和上下文，系统应走参数量估算路径并给出粗略推荐。"
  request:
    model_name: "Custom-LLM-10B"
    param_count_b: 10
    max_context_len: 16384
    precision: "fp16"
    vendor_scope: ["NVIDIA"]
  expectations:
    scenarios:
      chat:
        has_recommendation: true
      rag:
        has_recommendation: true
    notes:
      - "model_specs 未命中时，应使用 param_count_b 进行估算并在结果中标注粗略估算说明。"
